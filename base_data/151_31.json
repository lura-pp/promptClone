{
    "title": "Web Crawl and Index Superprompt",
    "original_prompt": "Act as prompt generator, I will give you a programming language, and you will provide a detailed prompt for me to choose. You will create prompts to crawl and index websites for a specific search term. \n\nPlease adhere to the structure and formatting below, and follow these guidelines:\n- wait for my target website, search term, preferred programming language, and indexing method strategy first.\n- write each prompt in one line without using return.\n- separate different prompts with two new lines.\n- choose maximum of 5 functions in [3].\n- choose maximum of 5 website crawling strategy in [4].\n- choose indexing method strategy based on [C], e.g. \"basic: Simple way of storing and retrieving data, such as storing data in a database or saving it to a file with minimal configuration.\", \"intermediate: More advanced way of storing and retrieving data, often requiring more configuration and setup, such as indexing data with Elasticsearch or using a third-party indexing service.\", \"advanced: The most complex and sophisticated way of storing and retrieving data, which may involve using machine learning algorithms or distributed computing frameworks to process large amounts of data in parallel.\"\n- write your prompts in English.\n\nStructure:\n[1] = preferred programming language. wait for my input.\n[2] = library in [1] for crawl and index websites.\n[3] = function specific for library [2] for crawl and index websites. separate each function with comma.\n[4] = step by step crawling strategy specific for library [2] in [1] programming language, separated by comma.\n[A] = target website. wait for my input.\n[B] = search term. wait for my input.\n[C] = type of project that will be suited as parameter for indexing method. you need to ask whether the project is either \"basic\", \"intermediate\", or \"advanced\". wait for my input. \n\nFormatting: \nFollow this prompt structure: '\"Create a tutorial on building a [1] script that leverages [2] to crawl and index website '[A]' for '[B]'. Explain the process step-by-step, and provide code samples highlighting the use of key components, such as `[3]`, website crawling strategy: '[4]', and indexing method : '[C]'. Include practical examples showcasing how different variables, like 'target website: `[A]`' and 'search term to be crawl and index: `[B]`', can be tailored for various requirements. Additionally, discuss potential challenges and best practices to optimize the performance of the script.\"'\n\nYour task: \n1. ask for preferred programming language ([1]), my target website ([A]), search term ([B]), and the project complexity ([C]).\n2. do not proceed before you got target website, search term, preferred programming language, and project complexity. \n3. then create 4 distinct prompts for each target website [A], varying in library, functions, crawling strategy, and indexing method regarding to the type of project.\n\nExample input: 'javascript, www.wikipedia.com, artificial intelligence, intermediate'.\nExample Prompt: '\"Create a tutorial on building a javascript script that leverages Puppeteer to crawl and index website 'www.wikipedia.com' for 'artificial intelligence'. Explain the process step-by-step, and provide code samples highlighting the use of key components, such as `puppeteer.launch(), page.goto(url), page.waitForSelector(selector), page.evaluate(() => { ... }), page.screenshot({path: 'example.png'}), page.content(), page.click(selector), page.goBack(), page.keyboard.type(text)`, website crawling strategy: 'Start with the root URL, use depth-first search to navigate to all pages, extract data using Puppeteer API, use site-specific search to focus on relevant pages or categories, repeat until all data is extracted.', and indexing method : 'index data with Elasticsearch'. Include practical examples showcasing how different variables, like 'target website: `www.wikipedia.com`' and 'search term to be crawl and index: `artificial intelligence`', can be tailored for various requirements. Additionally, discuss potential challenges and best practices to optimize the performance of the script.\"'",
    "split_prompt": "This is a prompt description:Please provide your preferred programming language ([1]), target website ([A]), search term ([B]), and the project complexity ([C]).",
    "input1": " 'python, www.example.com, machine learning, basic'  \n\n",
    "input2": " 'javascript, www.samplewebsite.com, web development, advanced'",
    "input1_ans": [
        "Please provide your preferred programming language ([1]), target website ([A]), search term ([B]), and the project complexity ([C]).",
        "Based on your input, here's a simple project outline for web scraping using Python to search for machine learning content on the website www.example.com.\n\n### Project Outline\n\n**Programming Language:** Python  \n**Target Website:** www.example.com  \n**Search Term:** machine learning  \n**Project Complexity:** Basic  \n\n### Steps to Implement\n\n1. **Set Up Your Environment:**\n   - Install Python if you haven't already.\n   - Install necessary libraries using pip:\n     ```bash\n     pip install requests beautifulsoup4\n     ```\n\n2. **Web Scraping Script:**\n   - Create a Python script (e.g., `scrape_ml.py`).\n\n3. **Code Example:**\n   ```python\n   import requests\n   from bs4 import BeautifulSoup\n\n   # Define the URL and search term\n   url = 'http://www.example.com'\n   search_term = 'machine learning'\n\n   # Send a GET request to the website\n   response = requests.get(url)\n\n   # Check if the request was successful\n   if response.status_code == 200:\n       # Parse the content of the page\n       soup = BeautifulSoup(response.text, 'html.parser')\n\n       # Find all instances of the search term\n       results = soup.body(text=lambda text: text and search_term in text.lower())\n\n       # Print the results\n       for result in results:\n           print(result)\n   else:\n       print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n   ```\n\n4. **Run the Script:**\n   - Execute the script in your terminal:\n     ```bash\n     python scrape_ml.py\n     ```\n\n5. **Review the Output:**\n   - The script will print out any text from the webpage that contains the term \"machine learning\".\n\n### Notes:\n- Make sure to check the website's `robots.txt` file to ensure that web scraping is allowed.\n- This is a basic example; you can expand it by adding error handling, saving results to a file, or scraping multiple pages.\n\nFeel free to modify the script according to your needs!"
    ],
    "input2_ans": [
        "Please provide the preferred programming language, target website, search term, and project complexity.",
        "Based on your input, here's a project outline for web scraping using JavaScript on the target website www.samplewebsite.com with a focus on web development, categorized as an advanced project.\n\n### Project Outline: Web Scraping with JavaScript\n\n#### 1. **Project Setup**\n   - **Environment**: Node.js\n   - **Dependencies**: \n     - `axios` for making HTTP requests\n     - `cheerio` for parsing HTML\n     - `puppeteer` for headless browser automation (if needed for dynamic content)\n\n#### 2. **Define Objectives**\n   - Scrape specific data related to web development from www.samplewebsite.com.\n   - Store the scraped data in a structured format (e.g., JSON, CSV, or a database).\n\n#### 3. **Identify Target Data**\n   - Determine the specific elements to scrape (e.g., articles, tutorials, code snippets).\n   - Inspect the website's HTML structure to identify the relevant tags and classes.\n\n#### 4. **Implement Web Scraping Logic**\n   - **Static Content Scraping**:\n     - Use `axios` to fetch the HTML content of the target pages.\n     - Use `cheerio` to parse the HTML and extract the desired data.\n   - **Dynamic Content Scraping** (if applicable):\n     - Use `puppeteer` to navigate the website and interact with dynamic elements (e.g., loading more content).\n\n#### 5. **Data Storage**\n   - Choose a storage method (e.g., local JSON file, MongoDB, or SQL database).\n   - Implement logic to save the scraped data in the chosen format.\n\n#### 6. **Error Handling and Logging**\n   - Implement error handling for network requests and data extraction.\n   - Set up logging to track the scraping process and any issues encountered.\n\n#### 7. **Testing and Validation**\n   - Test the scraper on various pages to ensure it captures all required data.\n   - Validate the data for accuracy and completeness.\n\n#### 8. **Optimization and Performance**\n   - Optimize the scraping process to handle rate limits and avoid being blocked by the website.\n   - Consider implementing a delay between requests.\n\n#### 9. **Documentation**\n   - Document the code and provide instructions on how to run the scraper.\n   - Include information on how to modify the scraper for different data or websites.\n\n#### 10. **Future Enhancements**\n   - Consider adding features like scheduling the scraper to run at regular intervals.\n   - Implement a user interface for easier interaction with the scraper.\n\n### Conclusion\nThis advanced web scraping project using JavaScript will allow you to gather valuable data from www.samplewebsite.com related to web development. Ensure to comply with the website's terms of service and robots.txt file to avoid any legal issues."
    ]
}